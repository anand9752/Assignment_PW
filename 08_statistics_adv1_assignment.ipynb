{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions for Probability Theory and Statistics Questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: What is a random variable in probability theory?\n",
    "\n",
    "A random variable is a variable whose possible values are numerical outcomes of a random phenomenon. It is a function that assigns a real number to each outcome in the sample space of a random experiment. Random variables provide a mathematical framework for quantifying uncertainty and variability in probabilistic scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: What are the types of random variables?\n",
    "\n",
    "Random variables can be classified into two main types:\n",
    "\n",
    "1. **Discrete Random Variables**: Take on countable, distinct values (like integers). Examples include the number of students in a class or the count of successful trials in an experiment.\n",
    "\n",
    "2. **Continuous Random Variables**: Can take any value within a range or interval on the real number line. Examples include height, weight, time, and temperature.\n",
    "\n",
    "There are also mixed random variables that have both discrete and continuous components, but they are less common in introductory statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: What is the difference between discrete and continuous distributions?\n",
    "\n",
    "The key differences between discrete and continuous distributions are:\n",
    "\n",
    "1. **Value Space**:\n",
    "   - Discrete distributions: Deal with countable, separated values (typically integers)\n",
    "   - Continuous distributions: Deal with uncountable values within a range\n",
    "\n",
    "2. **Probability Calculation**:\n",
    "   - Discrete: Probabilities are calculated for specific values (P(X = x))\n",
    "   - Continuous: Probabilities are calculated for ranges (P(a ≤ X ≤ b)) using integration\n",
    "\n",
    "3. **Graphical Representation**:\n",
    "   - Discrete: Represented by probability mass functions (PMFs) showing isolated points\n",
    "   - Continuous: Represented by probability density functions (PDFs) showing smooth curves\n",
    "\n",
    "4. **Point Probability**:\n",
    "   - Discrete: P(X = x) can be positive\n",
    "   - Continuous: P(X = x) = 0 for any specific point (probability exists only over intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: What are probability distribution functions (PDF)?\n",
    "\n",
    "A probability distribution function (PDF), also called a probability density function for continuous variables, is a function that describes the relative likelihood for a random variable to take on a given value. \n",
    "\n",
    "Key properties of PDFs:\n",
    "1. The function must be non-negative for all values in the domain\n",
    "2. The total area under the PDF curve must equal 1 (representing 100% probability)\n",
    "3. For continuous random variables, the probability of the variable falling within a specific range is the integral of the PDF over that range\n",
    "4. The PDF itself doesn't give probabilities directly but rather probability densities\n",
    "\n",
    "For discrete random variables, the equivalent concept is the probability mass function (PMF), which directly gives the probability of each possible value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: How do cumulative distribution functions (CDF) differ from probability distribution functions (PDF)?\n",
    "\n",
    "The key differences between CDFs and PDFs are:\n",
    "\n",
    "1. **Definition**:\n",
    "   - PDF: Gives the probability density at each point for continuous variables or probability mass for discrete variables\n",
    "   - CDF: Gives the probability that the random variable X is less than or equal to x, i.e., F(x) = P(X ≤ x)\n",
    "\n",
    "2. **Mathematical Relationship**:\n",
    "   - For continuous variables: CDF is the integral of the PDF\n",
    "   - For discrete variables: CDF is the sum of the PMF up to a given point\n",
    "   - Conversely, the PDF is the derivative of the CDF for continuous variables\n",
    "\n",
    "3. **Range of Values**:\n",
    "   - PDF: Can take any non-negative value (potentially greater than 1 for continuous variables)\n",
    "   - CDF: Always between 0 and 1, non-decreasing function\n",
    "\n",
    "4. **Practical Usage**:\n",
    "   - PDF: Used to understand the shape of distribution and find high-density regions\n",
    "   - CDF: Used for calculating probabilities for ranges and determining quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: What is a discrete uniform distribution?\n",
    "\n",
    "A discrete uniform distribution is a probability distribution where all outcomes in a finite set are equally likely to occur. \n",
    "\n",
    "Key characteristics:\n",
    "1. Every possible outcome has the same probability (1/n, where n is the number of possible outcomes)\n",
    "2. The random variable can take on a finite number of values\n",
    "3. The PMF is constant across all possible values\n",
    "4. Mean = (a + b)/2, where a is the minimum value and b is the maximum value\n",
    "5. Variance = ((b - a + 1)² - 1)/12\n",
    "\n",
    "Common examples include:\n",
    "- Rolling a fair die (uniform distribution over {1, 2, 3, 4, 5, 6})\n",
    "- Randomly selecting a card from a deck (uniform distribution over 52 possibilities)\n",
    "- Random number generators that produce integers in a specified range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: What are the key properties of a Bernoulli distribution?\n",
    "\n",
    "The Bernoulli distribution is the probability distribution of a random variable that takes the value 1 with probability p and the value 0 with probability (1-p). It models a single trial of a binary experiment.\n",
    "\n",
    "Key properties:\n",
    "1. Has only two possible outcomes, typically denoted as \"success\" (1) and \"failure\" (0)\n",
    "2. PMF: P(X = 1) = p and P(X = 0) = 1-p, where 0 ≤ p ≤ 1\n",
    "3. Mean (expected value) = p\n",
    "4. Variance = p(1-p)\n",
    "5. Standard deviation = √(p(1-p))\n",
    "6. Moment generating function: M(t) = (1-p) + pe^t\n",
    "7. It is the foundation for other distributions like the binomial and geometric distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: What is the binomial distribution, and how is it used in probability?\n",
    "\n",
    "The binomial distribution models the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success.\n",
    "\n",
    "Key characteristics:\n",
    "1. Represents the sum of n independent Bernoulli random variables\n",
    "2. Parameters: n (number of trials) and p (probability of success in a single trial)\n",
    "3. PMF: P(X = k) = C(n,k) * p^k * (1-p)^(n-k), where C(n,k) is the binomial coefficient\n",
    "4. Mean = np\n",
    "5. Variance = np(1-p)\n",
    "\n",
    "Common applications:\n",
    "- Quality control (number of defective items in a batch)\n",
    "- Medical testing (number of patients responding to treatment)\n",
    "- Polling and surveys (number of respondents with certain characteristics)\n",
    "- Genetics (inheritance patterns)\n",
    "- Risk analysis (number of successful outcomes in multiple attempts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9: What is the Poisson distribution and where is it applied?\n",
    "\n",
    "The Poisson distribution models the number of events occurring within a fixed interval of time or space, assuming these events happen at a constant average rate and independently of each other.\n",
    "\n",
    "Key characteristics:\n",
    "1. Parameter: λ (lambda) - the average number of events in the interval\n",
    "2. PMF: P(X = k) = (e^(-λ) * λ^k) / k! where k is a non-negative integer\n",
    "3. Mean = Variance = λ\n",
    "4. As n increases and p decreases in a binomial distribution while np remains constant, the binomial approaches a Poisson distribution\n",
    "\n",
    "Common applications:\n",
    "- Modeling rare events (number of accidents, equipment failures)\n",
    "- Customer arrival patterns (calls to a call center, visitors to a website)\n",
    "- Number of radioactive particle emissions in a time interval\n",
    "- Number of typing errors per page\n",
    "- Insurance claims in a time period\n",
    "- Mutations in DNA segments\n",
    "- Network traffic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10: What is a continuous uniform distribution?\n",
    "\n",
    "A continuous uniform distribution (also called a rectangular distribution) is a probability distribution where all intervals of equal length within the distribution's support have equal probability.\n",
    "\n",
    "Key characteristics:\n",
    "1. Parameters: a (minimum value) and b (maximum value)\n",
    "2. PDF: f(x) = 1/(b-a) for a ≤ x ≤ b, and 0 elsewhere\n",
    "3. CDF: F(x) = (x-a)/(b-a) for a ≤ x ≤ b\n",
    "4. Mean = (a+b)/2\n",
    "5. Variance = (b-a)²/12\n",
    "6. The PDF forms a rectangle with height 1/(b-a)\n",
    "\n",
    "Common applications:\n",
    "- Modeling rounding errors\n",
    "- Random number generation (when transformed, can generate other distributions)\n",
    "- Situations where any value in a range is equally likely\n",
    "- Models of uncertainty when only bounds are known\n",
    "- Simple approximations when the actual distribution is unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11: What are the characteristics of a normal distribution?\n",
    "\n",
    "The normal distribution (also called Gaussian distribution) is a continuous probability distribution that is symmetric around its mean and follows a bell-shaped curve.\n",
    "\n",
    "Key characteristics:\n",
    "1. Parameters: μ (mean) and σ (standard deviation)\n",
    "2. PDF: f(x) = (1/(σ√(2π))) * e^(-(x-μ)²/(2σ²))\n",
    "3. Symmetric about the mean (mean = median = mode)\n",
    "4. Bell-shaped curve with the highest point at the mean\n",
    "5. Asymptotic to the x-axis (approaches but never touches)\n",
    "6. 68-95-99.7 rule: approximately 68% of data falls within 1σ of the mean, 95% within 2σ, and 99.7% within 3σ\n",
    "7. Completely determined by its mean and standard deviation\n",
    "8. The sum or average of many independent random variables tends toward a normal distribution (Central Limit Theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12: What is the standard normal distribution, and why is it important?\n",
    "\n",
    "The standard normal distribution is a special case of the normal distribution with mean μ = 0 and standard deviation σ = 1. It is denoted as Z ~ N(0,1).\n",
    "\n",
    "Key points:\n",
    "1. PDF: f(z) = (1/√(2π)) * e^(-z²/2)\n",
    "2. Any normal random variable X can be transformed to a standard normal Z using the formula Z = (X-μ)/σ\n",
    "3. This transformation is called \"standardization\" or \"z-transformation\"\n",
    "\n",
    "Importance:\n",
    "1. Provides a standardized reference for all normal distributions\n",
    "2. Allows for easy calculation of probabilities using a single table or function\n",
    "3. Facilitates comparison between different normal distributions\n",
    "4. Fundamental to hypothesis testing and confidence intervals\n",
    "5. Used to calculate percentiles and critical values\n",
    "6. Forms the basis for many statistical tests and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13: What is the Central Limit Theorem (CLT), and why is it critical in statistics?\n",
    "\n",
    "The Central Limit Theorem (CLT) states that when independent random variables are added, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed.\n",
    "\n",
    "More specifically, if X₁, X₂, ..., Xₙ are independent and identically distributed random variables with mean μ and variance σ², then as n becomes large, the distribution of their sample mean X̄ approaches a normal distribution with mean μ and variance σ²/n.\n",
    "\n",
    "Critical importance in statistics:\n",
    "1. Enables statistical inference about populations without knowing the underlying distribution\n",
    "2. Justifies the use of normal-based procedures for large samples\n",
    "3. Forms the foundation for many hypothesis tests and confidence intervals\n",
    "4. Explains why many natural phenomena follow a normal distribution\n",
    "5. Allows for approximation of complex distributions when sample sizes are large\n",
    "6. Provides mathematical justification for sampling methods\n",
    "7. Forms the theoretical basis for quality control methods and experimental design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14: How does the Central Limit Theorem relate to the normal distribution?\n",
    "\n",
    "The relationship between the Central Limit Theorem (CLT) and the normal distribution is fundamental:\n",
    "\n",
    "1. **Convergence to Normal**: The CLT states that regardless of the original distribution of a population, the sampling distribution of the mean approaches a normal distribution as sample size increases.\n",
    "\n",
    "2. **Parameters of Resulting Distribution**: The resulting normal distribution has mean equal to the population mean μ and standard deviation equal to σ/√n (where σ is the population standard deviation and n is the sample size).\n",
    "\n",
    "3. **Rate of Convergence**: How quickly the sampling distribution approaches normality depends on the original distribution:\n",
    "   - If the original distribution is already normal, the sampling distribution is exactly normal for any sample size\n",
    "   - If the original distribution is symmetric, convergence happens more rapidly\n",
    "   - If the original distribution is highly skewed, larger sample sizes are needed\n",
    "\n",
    "4. **Universal Principle**: The CLT explains why the normal distribution is so prevalent in nature and statistics - many observations represent the sum of numerous small, independent effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15: What is the application of Z statistics in hypothesis testing?\n",
    "\n",
    "Z statistics are widely used in hypothesis testing, particularly when dealing with large samples or known population variances. Key applications include:\n",
    "\n",
    "1. **One-sample Z-test**: Tests whether a sample mean differs significantly from a known or hypothesized population mean when the population standard deviation is known.\n",
    "\n",
    "2. **Two-sample Z-test**: Compares means from two independent populations when both population standard deviations are known.\n",
    "\n",
    "3. **Z-test for proportions**: Tests hypotheses about population proportions for large samples.\n",
    "\n",
    "4. **Calculating p-values**: Z-scores are converted to p-values to determine statistical significance.\n",
    "\n",
    "5. **Decision making**: Comparing the calculated Z-statistic with critical Z-values to decide whether to reject the null hypothesis.\n",
    "\n",
    "6. **Power analysis**: Determining the sample size needed to detect an effect of a specified size with a given power.\n",
    "\n",
    "7. **Quality control**: Monitoring processes to detect significant deviations from target specifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16: How do you calculate a Z-score, and what does it represent?\n",
    "\n",
    "A Z-score (also called a standard score) represents how many standard deviations an observation is from the mean of a distribution.\n",
    "\n",
    "**Calculation**:\n",
    "For a single observation x from a population with mean μ and standard deviation σ:\n",
    "Z = (x - μ) / σ\n",
    "\n",
    "For a sample mean x̄ from a population with mean μ and standard error σ/√n:\n",
    "Z = (x̄ - μ) / (σ/√n)\n",
    "\n",
    "**What Z-scores represent**:\n",
    "1. **Standardized measure**: Converts values from any normal distribution to the standard normal distribution\n",
    "2. **Relative position**: Indicates where a value falls relative to the mean in terms of standard deviations\n",
    "3. **Probability interpretation**: Can be directly converted to probabilities using the standard normal table or function\n",
    "4. **Comparability**: Allows comparison of values from different distributions\n",
    "5. **Direction and magnitude**: Sign indicates direction (above/below mean); absolute value indicates distance\n",
    "   - Z = 0: Value equals the mean\n",
    "   - Z > 0: Value is above the mean\n",
    "   - Z < 0: Value is below the mean\n",
    "   - |Z| > 2: Value is unusual (falls outside ~95% of observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17: What are point estimates and interval estimates in statistics?\n",
    "\n",
    "**Point Estimates**:\n",
    "A point estimate is a single value (statistic) calculated from sample data that serves as the best guess or approximation of an unknown population parameter.\n",
    "\n",
    "Characteristics of point estimates:\n",
    "1. Provides a single value as an estimate\n",
    "2. Does not provide information about the precision or reliability of the estimate\n",
    "3. Common point estimates include:\n",
    "   - Sample mean (x̄) to estimate population mean (μ)\n",
    "   - Sample proportion (p̂) to estimate population proportion (p)\n",
    "   - Sample variance (s²) to estimate population variance (σ²)\n",
    "   - Sample median to estimate population median\n",
    "\n",
    "**Interval Estimates**:\n",
    "An interval estimate provides a range of values within which the population parameter is expected to lie with a specified level of confidence.\n",
    "\n",
    "Characteristics of interval estimates:\n",
    "1. Provides a range (interval) rather than a single value\n",
    "2. Includes a measure of reliability (confidence level)\n",
    "3. Indicates the precision of the estimate through the width of the interval\n",
    "4. Common interval estimates include:\n",
    "   - Confidence intervals for means, proportions, variances\n",
    "   - Prediction intervals for future observations\n",
    "   - Tolerance intervals for population percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 18: What is the significance of confidence intervals in statistical analysis?\n",
    "\n",
    "Confidence intervals are crucial in statistical analysis for several reasons:\n",
    "\n",
    "1. **Measurement of Precision**: They indicate the precision of a point estimate by providing a range of plausible values for the parameter.\n",
    "\n",
    "2. **Reliability Information**: The confidence level (typically 90%, 95%, or 99%) indicates the reliability of the estimation procedure.\n",
    "\n",
    "3. **Hypothesis Testing Alternative**: Confidence intervals can be used as an alternative to hypothesis testing - if a hypothesized value falls outside the interval, it would be rejected at the corresponding significance level.\n",
    "\n",
    "4. **Effect Size Assessment**: The width of the interval helps assess practical significance beyond statistical significance.\n",
    "\n",
    "5. **Uncertainty Communication**: They provide a clear way to communicate uncertainty in estimates to non-statisticians.\n",
    "\n",
    "6. **Sample Size Implications**: Wider intervals indicate greater uncertainty, often suggesting the need for larger sample sizes.\n",
    "\n",
    "7. **Decision Making**: They support evidence-based decision making by accounting for sampling variability.\n",
    "\n",
    "8. **Meta-analysis**: Confidence intervals facilitate the combination of results across multiple studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 19: What is the relationship between a Z-score and a confidence interval?\n",
    "\n",
    "The relationship between Z-scores and confidence intervals is fundamental in statistical inference:\n",
    "\n",
    "1. **Construction of Confidence Intervals**: Z-scores (critical values) are used to determine the width of confidence intervals. For a normal distribution, a confidence interval takes the form:\n",
    "   - Point estimate ± (Z-critical value × Standard error)\n",
    "\n",
    "2. **Confidence Level Determination**: The confidence level directly determines which Z-critical value to use:\n",
    "   - 90% confidence: Z ≈ 1.645\n",
    "   - 95% confidence: Z ≈ 1.96\n",
    "   - 99% confidence: Z ≈ 2.576\n",
    "\n",
    "3. **Probability Interpretation**: The Z-score defines the probability mass in the tails of the distribution that are excluded from the confidence interval.\n",
    "\n",
    "4. **Interval Width**: Larger Z-values (higher confidence levels) create wider intervals, reflecting greater certainty of capturing the true parameter.\n",
    "\n",
    "5. **Testing Equivalence**: A 95% confidence interval excludes values that would be rejected by a two-sided hypothesis test at α = 0.05.\n",
    "\n",
    "6. **Effect of Sample Size**: As sample size increases, the standard error decreases, narrowing the confidence interval without changing the Z-critical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 20: How are Z-scores used to compare different distributions?\n",
    "\n",
    "Z-scores are powerful tools for comparing values from different distributions:\n",
    "\n",
    "1. **Standardization**: By converting values to Z-scores, measurements from different distributions are placed on a common scale (standard normal distribution).\n",
    "\n",
    "2. **Relative Position Comparison**: Z-scores show the relative standing of values within their respective distributions:\n",
    "   - A score with Z = 1.5 is above average in its distribution by the same relative amount regardless of the original scale.\n",
    "\n",
    "3. **Performance Comparison**: Used to compare performance across different tests or metrics with different scales and distributions.\n",
    "\n",
    "4. **Outlier Identification**: Consistent definition of outliers across different data sets (e.g., |Z| > 3 is often considered an outlier).\n",
    "\n",
    "5. **Percentile Ranking**: Z-scores can be converted to percentiles, allowing comparison of relative standing across distributions.\n",
    "\n",
    "6. **Cross-Disciplinary Comparison**: Facilitates comparison of measurements from different fields that use different units or scales.\n",
    "\n",
    "7. **Data Normalization**: Used in machine learning and data analysis to normalize features with different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 21: What are the assumptions for applying the Central Limit Theorem?\n",
    "\n",
    "The Central Limit Theorem (CLT) requires certain assumptions for valid application:\n",
    "\n",
    "1. **Independence**: The sampled observations must be independent of each other.\n",
    "\n",
    "2. **Identically Distributed**: The observations should come from the same probability distribution (though some versions of CLT relax this requirement).\n",
    "\n",
    "3. **Finite Variance**: The population variance must be finite. If the variance is infinite, convergence to normality may not occur or may be extremely slow.\n",
    "\n",
    "4. **Sample Size**: The sample size (n) should be \"large enough\":\n",
    "   - For symmetric distributions, n ≥ 30 is often sufficient\n",
    "   - For moderately skewed distributions, n ≥ 50 may be needed\n",
    "   - For highly skewed distributions, n ≥ 100 or more might be required\n",
    "\n",
    "5. **Population Size**: For sampling without replacement from finite populations, the population size should be at least 10 times the sample size for the CLT to apply accurately.\n",
    "\n",
    "6. **Minimal Impact of Outliers**: Extreme outliers can affect the convergence rate, requiring larger sample sizes.\n",
    "\n",
    "Note that the CLT is robust to minor violations of these assumptions, particularly as sample size increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 22: What is the concept of expected value in a probability distribution?\n",
    "\n",
    "The expected value (also called the mean or expectation) of a random variable is the long-run average value of repetitions of the experiment it represents.\n",
    "\n",
    "Key aspects of expected value:\n",
    "\n",
    "1. **Mathematical Definition**:\n",
    "   - For discrete random variables: E(X) = Σ x·P(X = x)\n",
    "   - For continuous random variables: E(X) = ∫ x·f(x) dx\n",
    "\n",
    "2. **Interpretation**: It represents the theoretical average or center of mass of the probability distribution.\n",
    "\n",
    "3. **Properties**:\n",
    "   - Linearity: E(aX + b) = aE(X) + b\n",
    "   - Additivity: E(X + Y) = E(X) + E(Y) (regardless of independence)\n",
    "   - For independent variables: E(XY) = E(X)·E(Y)\n",
    "\n",
    "4. **Applications**:\n",
    "   - Decision theory: Expected payoff/utility calculations\n",
    "   - Insurance: Expected claim amounts\n",
    "   - Finance: Expected returns on investments\n",
    "   - Game theory: Expected gains/losses\n",
    "\n",
    "5. **Importance**: Forms the basis for calculating other moments of a distribution (variance, skewness, kurtosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 23: How does a probability distribution relate to the expected outcome of a random variable?\n",
    "\n",
    "A probability distribution and the expected outcome (expected value) of a random variable are integrally related:\n",
    "\n",
    "1. **Expected Value Definition**: The expected value is derived directly from the probability distribution by multiplying each possible value by its probability and summing (for discrete) or integrating (for continuous) over all possible values.\n",
    "\n",
    "2. **Location Parameter**: The expected value serves as a measure of central tendency or location of the probability distribution.\n",
    "\n",
    "3. **Prediction**: The expected value provides the best single-value prediction for the random variable in terms of minimizing mean squared error.\n",
    "\n",
    "4. **Relationship to Distribution Shape**:\n",
    "   - In symmetric distributions, the expected value coincides with the median and mode\n",
    "   - In right-skewed distributions, mean > median > mode\n",
    "   - In left-skewed distributions, mean < median < mode\n",
    "\n",
    "5. **Probabilistic Interpretation**: While the expected value is the long-run average, it may not be a value that can actually occur in the distribution (e.g., expected value of a fair die roll is 3.5).\n",
    "\n",
    "6. **Risk Assessment**: The difference between potential outcomes and the expected value forms the basis for risk measurement.\n",
    "\n",
    "7. **Law of Large Numbers**: As the number of trials increases, the average of observed values converges to the expected value, connecting theoretical probability to empirical frequency."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
