{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aSYxTwcS4vL9"
      },
      "outputs": [],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyWHhGzm43zR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ee8225c"
      },
      "source": [
        "### What is a Support Vector Machine (SVM)?\n",
        "\n",
        "A Support Vector Machine (SVM) is a powerful and versatile supervised machine learning algorithm used for both classification and regression tasks. However, it is most commonly used in classification problems.\n",
        "\n",
        "The main idea behind SVM is to find a hyperplane that best separates the data points of different classes in a high-dimensional space. The \"best\" hyperplane is the one that has the largest margin, which is the distance between the hyperplane and the nearest data points from each class. These nearest data points are called \"support vectors,\" and they are the critical elements of the dataset that define the decision boundary.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "*   **Hyperplane:** In a 2D space, a hyperplane is a line. In a 3D space, it's a plane, and in higher dimensions, it's a hyperplane. It's the decision boundary that separates the classes.\n",
        "*   **Margin:** The distance between the hyperplane and the closest data points from either class. SVM aims to maximize this margin.\n",
        "*   **Support Vectors:** The data points that are closest to the hyperplane and influence its position and orientation. If these points were to be removed, the position of the hyperplane would change.\n",
        "\n",
        "**Why is maximizing the margin important?**\n",
        "\n",
        "A larger margin indicates a more confident and robust model that is less likely to misclassify new, unseen data points. It creates a clear separation between the classes, leading to better generalization performance.\n",
        "\n",
        "**Types of SVM:**\n",
        "\n",
        "*   **Linear SVM:** Used when the data is linearly separable.\n",
        "*   **Non-linear SVM:** Used when the data is not linearly separable. It uses the \"kernel trick\" to map the data into a higher-dimensional space where it becomes linearly separable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92992445"
      },
      "source": [
        "### What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "The difference between Hard Margin and Soft Margin SVM lies in how they handle misclassifications and outliers in the training data.\n",
        "\n",
        "**Hard Margin SVM:**\n",
        "\n",
        "*   **Strict Separation:** Hard Margin SVM requires that all data points be classified correctly and placed outside the margin. It does not tolerate any misclassifications or data points within the margin.\n",
        "*   **Linearly Separable Data:** This approach only works when the data is perfectly linearly separable. If the data is not linearly separable, a Hard Margin SVM cannot find a solution.\n",
        "*   **Sensitivity to Outliers:** Hard Margin SVM is very sensitive to outliers. A single outlier can significantly affect the position of the hyperplane and the margin, leading to a less optimal solution.\n",
        "\n",
        "**Soft Margin SVM:**\n",
        "\n",
        "*   **Flexible Separation:** Soft Margin SVM allows for some misclassifications and data points within the margin. It introduces a \"slack variable\" that allows some data points to be on the wrong side of the margin or even on the wrong side of the hyperplane.\n",
        "*   **Handles Non-linearly Separable Data:** Soft Margin SVM can handle data that is not linearly separable by allowing for some errors.\n",
        "*   **Robust to Outliers:** By allowing for some misclassifications, Soft Margin SVM is more robust to outliers. It can find a hyperplane that is not overly influenced by a few noisy data points.\n",
        "*   **The C Parameter:** Soft Margin SVM introduces a hyperparameter \"C\" that controls the trade-off between maximizing the margin and minimizing the classification error.\n",
        "    *   A **small C** value creates a wider margin but allows for more misclassifications. This can lead to a more generalized model.\n",
        "    *   A **large C** value creates a narrower margin and penalizes misclassifications more heavily. This can lead to a model that overfits the training data.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "| Feature | Hard Margin SVM | Soft Margin SVM |\n",
        "| :--- | :--- | :--- |\n",
        "| **Misclassifications**| Not allowed | Allowed |\n",
        "| **Data Separability**| Must be linearly separable | Can be non-linearly separable |\n",
        "| **Outlier Sensitivity**| High | Low |\n",
        "| **Flexibility** | Low | High |\n",
        "| **Use Case** | Ideal, perfectly separable data | Real-world, noisy data |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "242c3a2b"
      },
      "source": [
        "### What is the mathematical intuition behind SVM?\n",
        "\n",
        "The mathematical intuition behind SVM is to find the optimal hyperplane that maximizes the margin between the two classes. This is formulated as a constrained optimization problem.\n",
        "\n",
        "**1. The Hyperplane Equation:**\n",
        "\n",
        "A hyperplane in an n-dimensional space can be represented by the equation:\n",
        "\n",
        "**w.x + b = 0**\n",
        "\n",
        "where:\n",
        "*   **w** is the weight vector (normal to the hyperplane).\n",
        "*   **x** is the input feature vector.\n",
        "*   **b** is the bias term.\n",
        "\n",
        "**2. The Margin:**\n",
        "\n",
        "The margin is the distance between the two parallel hyperplanes that are closest to the data points of each class. These hyperplanes can be represented as:\n",
        "\n",
        "*   **w.x + b = 1** (for the positive class)\n",
        "*   **w.x + b = -1** (for the negative class)\n",
        "\n",
        "The distance between these two hyperplanes is **2 / ||w||**, where **||w||** is the norm (magnitude) of the weight vector.\n",
        "\n",
        "**3. The Optimization Problem:**\n",
        "\n",
        "The goal of SVM is to maximize the margin, which is equivalent to minimizing **||w||**. This is formulated as the following optimization problem:\n",
        "\n",
        "**Minimize (1/2) * ||w||^2**\n",
        "\n",
        "This is a quadratic programming problem, which is a type of convex optimization problem that has a unique solution.\n",
        "\n",
        "**4. The Constraints:**\n",
        "\n",
        "The optimization problem is subject to the following constraints, which ensure that all data points are correctly classified:\n",
        "\n",
        "*   **w.x_i + b >= 1** for all data points **x_i** in the positive class.\n",
        "*   **w.x_i + b <= -1** for all data points **x_i** in the negative class.\n",
        "\n",
        "These two constraints can be combined into a single constraint:\n",
        "\n",
        "**y_i * (w.x_i + b) >= 1** for all i\n",
        "\n",
        "where:\n",
        "*   **y_i** is the class label of the i-th data point (+1 for the positive class, -1 for the negative class).\n",
        "\n",
        "**In summary, the mathematical intuition behind SVM is to find the weight vector w and bias term b that minimize the magnitude of w (and thus maximize the margin) while ensuring that all data points are correctly classified.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1841a2be"
      },
      "source": [
        "### What is the role of Lagrange Multipliers in SVM?\n",
        "\n",
        "Lagrange Multipliers play a crucial role in solving the constrained optimization problem of SVM. They allow us to transform the original problem (the primal problem) into a new, easier-to-solve problem (the dual problem).\n",
        "\n",
        "**The Primal Problem:**\n",
        "\n",
        "The primal problem of SVM is to minimize **(1/2) * ||w||^2** subject to the constraint **y_i * (w.x_i + b) >= 1**.\n",
        "\n",
        "This is a constrained optimization problem that can be difficult to solve directly, especially in high-dimensional spaces.\n",
        "\n",
        "**The Lagrangian Formulation:**\n",
        "\n",
        "To solve this, we introduce Lagrange multipliers (α_i) for each constraint. The Lagrangian function is:\n",
        "\n",
        "**L(w, b, α) = (1/2) * ||w||^2 - Σ α_i * [y_i * (w.x_i + b) - 1]**\n",
        "\n",
        "where:\n",
        "*   **α_i** are the Lagrange multipliers.\n",
        "*   **α_i >= 0**\n",
        "\n",
        "**The Dual Problem:**\n",
        "\n",
        "The dual problem is to maximize the Lagrangian function with respect to α, subject to certain constraints. The dual problem is easier to solve because it only depends on the Lagrange multipliers α and not on the weight vector w.\n",
        "\n",
        "**The Role of Lagrange Multipliers:**\n",
        "\n",
        "1.  **Transforming the Problem:** Lagrange multipliers allow us to transform the primal problem into the dual problem, which is a simpler quadratic programming problem.\n",
        "\n",
        "2.  **Identifying Support Vectors:** The Lagrange multipliers have a very important property: they are non-zero only for the support vectors.\n",
        "    *   If **α_i > 0**, then the i-th data point is a support vector.\n",
        "    *   If **α_i = 0**, then the i-th data point is not a support vector.\n",
        "\n",
        "3.  **Making Predictions:** The decision function for a new data point x can be expressed in terms of the Lagrange multipliers and the support vectors:\n",
        "\n",
        "    **f(x) = sign(Σ α_i * y_i * (x_i . x) + b)**\n",
        "\n",
        "    This means that we only need the support vectors to make predictions, which makes SVM very efficient, especially for large datasets.\n",
        "\n",
        "**In summary, Lagrange multipliers are a mathematical tool that allows us to solve the SVM optimization problem efficiently and identify the support vectors, which are the most important data points for defining the decision boundary.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02406c99"
      },
      "source": [
        "### What are Support Vectors in SVM?\n",
        "\n",
        "In Support Vector Machines, **support vectors** are the data points that lie closest to the decision boundary (the hyperplane). They are the critical data points that \"support\" the hyperplane and determine its position and orientation.\n",
        "\n",
        "**Key Characteristics of Support Vectors:**\n",
        "\n",
        "*   **Closest to the Hyperplane:** They are the data points that are nearest to the separating hyperplane.\n",
        "*   **On the Margin:** In a hard-margin SVM, the support vectors lie exactly on the margin. In a soft-margin SVM, they can be on the margin or within the margin (or even misclassified).\n",
        "*   **Define the Decision Boundary:** If you were to move a support vector, the position of the hyperplane would change. Conversely, if you were to move a data point that is not a support vector, the hyperplane would not change (as long as the point doesn't cross the margin).\n",
        "*   **Difficult to Classify:** They are the most difficult data points to classify because they are so close to the decision boundary.\n",
        "\n",
        "**Why are they important?**\n",
        "\n",
        "Support vectors are crucial to SVM for several reasons:\n",
        "\n",
        "1.  **Efficiency:** SVM is a very efficient algorithm because it only uses the support vectors to create the decision boundary, not the entire dataset. This makes it computationally less expensive, especially for large datasets.\n",
        "2.  **Robustness:** By focusing on the data points that are most difficult to classify, SVM creates a decision boundary that is robust and generalizes well to new, unseen data.\n",
        "3.  **Sparsity:** The fact that the decision boundary is determined by a small subset of the data (the support vectors) is known as sparsity. This is a desirable property in machine learning models.\n",
        "\n",
        "**In summary, support vectors are the cornerstone of the SVM algorithm. They are the data points that define the decision boundary and make SVM a powerful and efficient classification algorithm.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2f35a42"
      },
      "source": [
        "### What is a Support Vector Classifier (SVC)?\n",
        "\n",
        "A Support Vector Classifier (SVC) is the specific implementation of the Support Vector Machine (SVM) algorithm for classification tasks. Its primary goal is to find a hyperplane that best separates data points belonging to different classes in a high-dimensional space.\n",
        "\n",
        "**Key characteristics of SVC:**\n",
        "\n",
        "*   **Classification:** As the name suggests, SVC is used for classification problems, where the goal is to assign a categorical label to an input data point.\n",
        "*   **Linear and Non-linear problems:** SVC can be used for both linearly separable and non-linearly separable data.\n",
        "    *   For linearly separable data, it finds a linear hyperplane.\n",
        "    *   For non-linearly separable data, it uses the \"kernel trick\" to map the data into a higher-dimensional space where it becomes linearly separable.\n",
        "*   **Binary and Multi-class classification:** SVC is naturally a binary classifier, meaning it separates two classes. However, it can be extended to handle multi-class classification problems using techniques like \"one-vs-one\" or \"one-vs-rest\".\n",
        "\n",
        "**In summary, when people refer to using SVM for classification, they are typically talking about using a Support Vector Classifier (SVC).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ee72a93"
      },
      "source": [
        "### What is a Support Vector Regressor (SVR)?\n",
        "\n",
        "A Support Vector Regressor (SVR) is the implementation of the Support Vector Machine (SVM) algorithm for regression tasks. Unlike SVC, which predicts a categorical label, SVR predicts a continuous value.\n",
        "\n",
        "**The main idea behind SVR is to find a hyperplane that has the maximum number of data points within a certain margin (ε-tube).**\n",
        "\n",
        "**Key characteristics of SVR:**\n",
        "\n",
        "*   **Regression:** SVR is used for regression problems, where the goal is to predict a continuous output value.\n",
        "*   **Margin of Tolerance (ε-tube):** SVR tries to fit as many data points as possible within a specified margin of tolerance (the ε-tube). Data points within this tube are not penalized.\n",
        "*   **Support Vectors:** The data points that lie outside the ε-tube are the support vectors. These are the points that influence the position of the hyperplane.\n",
        "*   **Linear and Non-linear problems:** Like SVC, SVR can be used for both linear and non-linear regression problems using the kernel trick.\n",
        "\n",
        "**In summary, SVR is the regression counterpart of SVC. While SVC aims to separate classes with a maximal margin, SVR aims to fit a regression line that has the maximum number of data points within a certain margin.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d2905c6"
      },
      "source": [
        "### What is the Kernel Trick in SVM?\n",
        "\n",
        "The kernel trick is a powerful technique that allows SVM to solve non-linear classification problems. It's a way of mapping the data into a higher-dimensional space where it becomes linearly separable, without having to explicitly compute the coordinates of the data in that new space.\n",
        "\n",
        "**The Problem with Non-linear Data:**\n",
        "\n",
        "Sometimes, data is not linearly separable in its original feature space. For example, imagine a dataset where the positive and negative classes are arranged in concentric circles. You cannot draw a straight line to separate them.\n",
        "\n",
        "**The Solution: The Kernel Trick**\n",
        "\n",
        "The kernel trick works by replacing the dot product of the input features in the SVM algorithm with a kernel function. A kernel function is a function that computes the dot product of the data points in a higher-dimensional space, without actually transforming the data into that space.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1.  **Choose a Kernel Function:** There are several types of kernel functions, such as the polynomial kernel, the radial basis function (RBF) kernel, and the sigmoid kernel.\n",
        "2.  **Apply the Kernel Function:** The kernel function is applied to the input data, which implicitly maps the data into a higher-dimensional space.\n",
        "3.  **Find the Hyperplane:** SVM then finds the optimal hyperplane in this new, higher-dimensional space.\n",
        "4.  **Make Predictions:** The decision boundary is then projected back into the original feature space, resulting in a non-linear decision boundary.\n",
        "\n",
        "**Why is this a \"trick\"?**\n",
        "\n",
        "The \"trick\" is that we never have to explicitly compute the coordinates of the data in the higher-dimensional space. This is computationally very efficient, especially for very high-dimensional spaces.\n",
        "\n",
        "**In summary, the kernel trick is a mathematical shortcut that allows SVM to learn complex, non-linear decision boundaries by implicitly mapping the data into a higher-dimensional space.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "437721d9"
      },
      "source": [
        "### Compare Linear Kernel, Polynomial Kernel, and RBF Kernel:\n",
        "\n",
        "| Feature | Linear Kernel | Polynomial Kernel | RBF Kernel |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Decision Boundary** | Linear | Polynomial | Complex, non-linear |\n",
        "| **Use Case** | Linearly separable data | Non-linearly separable data | Non-linearly separable data |\n",
        "| **Hyperparameters** | C | C, degree, coef0 | C, gamma |\n",
        "| **Computational Cost** | Low | Medium | High |\n",
        "| **Flexibility** | Low | Medium | High |\n",
        "| **Overfitting Risk** | Low | Medium | High |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53f92662"
      },
      "source": [
        "### What is the effect of the C parameter in SVM?\n",
        "\n",
        "The C parameter in SVM is a hyperparameter that controls the trade-off between maximizing the margin and minimizing the classification error. It is also known as the \"regularization parameter\".\n",
        "\n",
        "*   **Small C:** A small C value creates a wider margin but allows for more misclassifications. This can lead to a more generalized model that is less likely to overfit the training data.\n",
        "*   **Large C:** A large C value creates a narrower margin and penalizes misclassifications more heavily. This can lead to a model that overfits the training data, as it tries to classify every single data point correctly.\n",
        "\n",
        "**In summary, the C parameter is a crucial hyperparameter that needs to be tuned to find the right balance between a smooth decision boundary and accurate classification of the training data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a457a274"
      },
      "source": [
        "### What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "\n",
        "The gamma parameter in the RBF kernel SVM controls the shape of the decision boundary. It determines how much influence a single training example has.\n",
        "\n",
        "*   **Small Gamma:** A small gamma value means that a single training example has a large influence. The decision boundary will be very smooth and may not be able to capture the complexity of the data.\n",
        "*   **Large Gamma:** A large gamma value means that a single training example has a small influence. The decision boundary will be more complex and can lead to overfitting, as it tries to fit every single data point.\n",
        "\n",
        "**In summary, the gamma parameter is another important hyperparameter that needs to be tuned to find the right level of complexity for the decision boundary.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e7399f6"
      },
      "source": [
        "### What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "The Naïve Bayes classifier is a simple and effective probabilistic machine learning algorithm based on Bayes' theorem. It is a supervised learning algorithm that is commonly used for classification tasks, especially text classification.\n",
        "\n",
        "**Why is it called \"Naïve\"?**\n",
        "\n",
        "The \"naïve\" part of the name comes from the key assumption that the algorithm makes: **it assumes that all features are independent of each other, given the class.**\n",
        "\n",
        "For example, in a text classification task, the algorithm assumes that the presence of the word \"love\" in a document is independent of the presence of the word \"happy\", given that the document is a \"positive\" review.\n",
        "\n",
        "This is a \"naïve\" assumption because in reality, words are often not independent of each other. For example, the words \"love\" and \"happy\" are more likely to appear together in a positive review.\n",
        "\n",
        "However, despite this simplifying assumption, Naïve Bayes often performs surprisingly well in practice, especially for text classification problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d36dae06"
      },
      "source": [
        "### What is Bayes’ Theorem?\n",
        "\n",
        "Bayes' theorem is a fundamental theorem in probability theory that describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n",
        "\n",
        "The formula for Bayes' theorem is:\n",
        "\n",
        "**P(A|B) = [P(B|A) * P(A)] / P(B)**\n",
        "\n",
        "where:\n",
        "\n",
        "*   **P(A|B)** is the posterior probability: the probability of event A occurring, given that event B has occurred.\n",
        "*   **P(B|A)** is the likelihood: the probability of event B occurring, given that event A has occurred.\n",
        "*   **P(A)** is the prior probability: the probability of event A occurring.\n",
        "*   **P(B)** is the marginal probability: the probability of event B occurring.\n",
        "\n",
        "**In the context of Naïve Bayes, we use Bayes' theorem to calculate the probability of a class, given a set of features.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31f097f2"
      },
      "source": [
        "### Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes:\n",
        "\n",
        "| Feature | Gaussian Naïve Bayes | Multinomial Naïve Bayes | Bernoulli Naïve Bayes |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Feature Type** | Continuous | Discrete (counts) | Binary (presence/absence) |\n",
        "| **Data Distribution** | Assumes features follow a Gaussian (normal) distribution | Assumes features follow a multinomial distribution | Assumes features follow a Bernoulli distribution |\n",
        "| **Use Case** | Real-valued features (e.g., height, weight) | Text classification (word counts) | Text classification (word presence/absence) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc61116a"
      },
      "source": [
        "### When should you use Gaussian Naïve Bayes over other variants?\n",
        "\n",
        "You should use Gaussian Naïve Bayes when your features are continuous and you believe that they follow a Gaussian (normal) distribution.\n",
        "\n",
        "For example, if you are trying to predict whether a person is male or female based on their height and weight, you would use Gaussian Naïve Bayes because height and weight are continuous features that are likely to be normally distributed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ada1f87a"
      },
      "source": [
        "### What are the key assumptions made by Naïve Bayes?\n",
        "\n",
        "The key assumptions made by Naïve Bayes are:\n",
        "\n",
        "1.  **Independence of Features:** This is the \"naïve\" assumption that all features are independent of each other, given the class.\n",
        "2.  **Equal Importance of Features:** Naïve Bayes assumes that all features are equally important in determining the class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58618b7e"
      },
      "source": [
        "### What are the advantages and disadvantages of Naïve Bayes?\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "*   **Simple and Fast:** It is a very simple and fast algorithm to train and make predictions with.\n",
        "*   **Works well with high-dimensional data:** It can handle a large number of features, which makes it suitable for text classification problems.\n",
        "*   **Requires a small amount of training data:** It can perform well even with a small amount of training data.\n",
        "*   **Handles both continuous and discrete data:** There are different variants of Naïve Bayes that can handle different types of data.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "*   **The \"naïve\" assumption of feature independence:** This assumption is often violated in real-world data, which can lead to suboptimal performance.\n",
        "*   **The \"zero-frequency problem\":** If a categorical variable has a category in the test data set, which was not observed in the training data set, then the model will assign a 0 probability and will be unable to make a prediction. This is often handled using smoothing techniques like Laplace smoothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12a65e77"
      },
      "source": [
        "### Why is Naïve Bayes a good choice for text classification?\n",
        "\n",
        "Naïve Bayes is a good choice for text classification for several reasons:\n",
        "\n",
        "*   **Handles high-dimensional data:** Text data is typically very high-dimensional, as the number of features is equal to the size of the vocabulary. Naïve Bayes can handle this high dimensionality without any problems.\n",
        "*   **The \"naïve\" assumption is not a major issue:** In text classification, the assumption of feature independence is not as big of an issue as it might seem. While words are not truly independent, the assumption is often \"good enough\" to achieve good performance.\n",
        "*   **It is fast and efficient:** Naïve Bayes is a very fast algorithm to train and make predictions with, which is important for large text datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "137a1f08"
      },
      "source": [
        "### Compare SVM and Naïve Bayes for classification tasks:\n",
        "\n",
        "| Feature | SVM | Naïve Bayes |\n",
        "| :--- | :--- | :--- |\n",
        "| **Performance** | Generally performs better, especially with complex data | Can perform surprisingly well, especially for text classification |\n",
        "| **Speed** | Slower to train, especially with large datasets | Faster to train and make predictions |\n",
        "| **Data Requirements** | Requires more training data | Can work well with small amounts of training data |\n",
        "| **Interpretability** | Less interpretable | More interpretable |\n",
        "| **Hyperparameters** | Requires tuning of hyperparameters (C, gamma) | Fewer hyperparameters to tune |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c07897b"
      },
      "source": [
        "### How does Laplace Smoothing help in Naïve Bayes?\n",
        "\n",
        "Laplace smoothing, also known as additive smoothing, is a technique used to handle the problem of zero probabilities in Naïve Bayes.\n",
        "\n",
        "**The Problem of Zero Probabilities**\n",
        "\n",
        "In Naïve Bayes, we calculate the probability of a feature given a class. For example, in a text classification task, we might calculate the probability of the word \"love\" appearing in a \"positive\" review.\n",
        "\n",
        "If a particular word never appears in the training data for a specific class, the conditional probability of that word given that class will be zero. This creates a problem because when we multiply all the conditional probabilities together to calculate the final probability of the class, the entire expression becomes zero. This means that even if all other words in the text strongly suggest a particular class, the presence of a single unseen word will make the probability of that class zero.\n",
        "\n",
        "**How Laplace Smoothing Helps**\n",
        "\n",
        "Laplace smoothing solves this problem by adding a small positive value (usually 1) to the count of each word in the vocabulary for each class. This ensures that no word has a zero probability.\n",
        "\n",
        "**Here's how it works:**\n",
        "\n",
        "Let's say we have:\n",
        "\n",
        "*   `count(word, class)`: The number of times a word appears in documents of a particular class.\n",
        "*   `total_words(class)`: The total number of words in all documents of that class.\n",
        "*   `V`: The total number of unique words in the vocabulary.\n",
        "\n",
        "**Without Laplace Smoothing:**\n",
        "\n",
        "P(word | class) = `count(word, class)` / `total_words(class)`\n",
        "\n",
        "If `count(word, class)` is 0, then P(word | class) is 0.\n",
        "\n",
        "**With Laplace Smoothing:**\n",
        "\n",
        "P(word | class) = (`count(word, class)` + 1) / (`total_words(class)` + `V`)\n",
        "\n",
        "By adding 1 to the numerator, we ensure that the probability is never zero. We add `V` to the denominator to normalize the probabilities so that they still sum to 1.\n",
        "\n",
        "**In essence, Laplace smoothing pretends that we have seen each word in the vocabulary one more time than we actually have. This prevents the model from being overly confident about the absence of a word and makes it more robust to unseen data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUYL-Xc2PDuG"
      },
      "source": [
        "## Q. 21 Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d6aba36",
        "outputId": "4297a33c-e1e4-4e26-c06e-29096b0d7918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create an SVM classifier with a linear kernel\n",
        "clf = SVC(kernel='linear')\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxF41uACPJHA"
      },
      "source": [
        "## Q. 22 Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2681081",
        "outputId": "e9976889-3b75-4ab1-b4e2-66dff90e7f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (Linear Kernel): 0.9814814814814815\n",
            "Accuracy (RBF Kernel): 0.7592592592592593\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create an SVM classifier with a linear kernel\n",
        "clf_linear = SVC(kernel='linear')\n",
        "\n",
        "# Train the linear classifier\n",
        "clf_linear.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the linear classifier\n",
        "y_pred_linear = clf_linear.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the linear classifier\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "print(f\"Accuracy (Linear Kernel): {accuracy_linear}\")\n",
        "\n",
        "# Create an SVM classifier with an RBF kernel\n",
        "clf_rbf = SVC(kernel='rbf')\n",
        "\n",
        "# Train the RBF classifier\n",
        "clf_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the RBF classifier\n",
        "y_pred_rbf = clf_rbf.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the RBF classifier\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "print(f\"Accuracy (RBF Kernel): {accuracy_rbf}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEfXqC7UPTFb"
      },
      "source": [
        "## Q. 23 Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "# selecting a part of the dataset 200 samples random\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "indices = np.random.choice(len(housing.data), size=200, replace=False)\n",
        "X = housing.data[indices]\n",
        "y = housing.target[indices]\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create an SVR model\n",
        "svr = SVR(kernel='linear')\n",
        "\n",
        "# Train the model\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q. 24 Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "df682d53"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # We only take the first two features for visualization\n",
        "y = iris.target\n",
        "\n",
        "# Create an SVM classifier with a polynomial kernel\n",
        "clf = SVC(kernel='poly', degree=3)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Create a mesh to plot in\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "# Plot the decision boundary\n",
        "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "\n",
        "# Plot also the training points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "plt.title('SVM with Polynomial Kernel')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDe64KF-QJRs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q. 25 Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Information:\n",
            "Features: 30\n",
            "Target classes: ['malignant' 'benign']\n",
            "Dataset shape: (569, 30)\n",
            "Accuracy: 0.9737\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "print(\"Dataset Information:\")\n",
        "print(f\"Features: {len(cancer.feature_names)}\")\n",
        "print(f\"Target classes: {cancer.target_names}\")\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=cancer.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q. 26 Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load the 20 Newsgroups dataset (subset for faster computation)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m categories \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malt.atheism\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoc.religion.christian\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomp.graphics\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msci.med\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m newsgroups_train \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_20newsgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m newsgroups_test \u001b[38;5;241m=\u001b[39m fetch_20newsgroups(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, categories\u001b[38;5;241m=\u001b[39mcategories, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset Information:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:320\u001b[0m, in \u001b[0;36mfetch_20newsgroups\u001b[1;34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y, n_retries, delay)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download_if_missing:\n\u001b[0;32m    319\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading 20news dataset. This may take a few minutes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 320\u001b[0m     cache \u001b[38;5;241m=\u001b[39m \u001b[43m_download_20newsgroups\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtwenty_home\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20Newsgroups dataset not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:93\u001b[0m, in \u001b[0;36m_download_20newsgroups\u001b[1;34m(target_dir, cache_path, n_retries, delay)\u001b[0m\n\u001b[0;32m     88\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(archive_path)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Store a zipped pickle\u001b[39;00m\n\u001b[0;32m     91\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     92\u001b[0m     train\u001b[38;5;241m=\u001b[39mload_files(train_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m---> 93\u001b[0m     test\u001b[38;5;241m=\u001b[39m\u001b[43mload_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     94\u001b[0m )\n\u001b[0;32m     95\u001b[0m compressed_content \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mencode(pickle\u001b[38;5;241m.\u001b[39mdumps(cache), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzlib_codec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cache_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\datasets\\_base.py:309\u001b[0m, in \u001b[0;36mload_files\u001b[1;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state, allowed_extensions)\u001b[0m\n\u001b[0;32m    307\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m filenames:\n\u001b[1;32m--> 309\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     data \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mdecode(encoding, decode_error) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1520.0_x64__qbz5n2kfra8p0\\Lib\\pathlib\\_abc.py:625\u001b[0m, in \u001b[0;36mPathBase.read_bytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    622\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;124;03m    Open the file in bytes mode, read it, and close the file.\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    626\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1520.0_x64__qbz5n2kfra8p0\\Lib\\pathlib\\_local.py:537\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    536\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m--> 537\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Load the 20 Newsgroups dataset (subset for faster computation)\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"Dataset Information:\")\n",
        "print(f\"Number of training samples: {len(newsgroups_train.data)}\")\n",
        "print(f\"Number of test samples: {len(newsgroups_test.data)}\")\n",
        "print(f\"Categories: {newsgroups_train.target_names}\")\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "X_test = vectorizer.transform(newsgroups_test.data)\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "print(f\"Feature matrix shape: {X_train.shape}\")\n",
        "\n",
        "# Train Multinomial Naive Bayes classifier\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = mnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=newsgroups_test.target_names))\n",
        "\n",
        "# Show some example predictions\n",
        "print(\"\\nExample Predictions:\")\n",
        "for i in range(5):\n",
        "    print(f\"Text: {newsgroups_test.data[i][:100]}...\")\n",
        "    print(f\"True Category: {newsgroups_test.target_names[y_test[i]]}\")\n",
        "    print(f\"Predicted Category: {newsgroups_test.target_names[y_pred[i]]}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q. 27 Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q. 28 Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with binary features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a binary dataset with binary features\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, \n",
        "                          n_redundant=0, n_classes=2, random_state=42)\n",
        "\n",
        "# Convert to binary features (0 or 1)\n",
        "X_binary = (X > 0).astype(int)\n",
        "\n",
        "print(\"Dataset Information:\")\n",
        "print(f\"Shape: {X_binary.shape}\")\n",
        "print(f\"Feature values: {np.unique(X_binary)}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_binary, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bernoulli Naive Bayes classifier\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Class 0', 'Class 1'], \n",
        "            yticklabels=['Class 0', 'Class 1'])\n",
        "plt.title('Confusion Matrix - Bernoulli Naive Bayes')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Show feature probabilities for each class\n",
        "print(\"\\nFeature Probabilities for each class:\")\n",
        "feature_probs = bnb.feature_log_prob_\n",
        "for i in range(2):\n",
        "    print(f\"Class {i}: {np.exp(feature_probs[i])[:5]}\")  # Show first 5 features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q. 29 Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a dataset with some noise for better demonstration\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, \n",
        "                          n_informative=2, n_clusters_per_class=1, \n",
        "                          random_state=42)\n",
        "\n",
        "# Add some noise to make the data slightly overlapping\n",
        "np.random.seed(42)\n",
        "X += np.random.normal(0, 0.1, X.shape)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Function to plot decision boundary\n",
        "def plot_svm_decision_boundary(X, y, model, title):\n",
        "    h = 0.02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
        "    \n",
        "    # Plot support vectors\n",
        "    if hasattr(model, 'support_vectors_'):\n",
        "        plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], \n",
        "                   s=100, facecolors='none', edgecolors='k', linewidths=2)\n",
        "    \n",
        "    plt.title(title)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "\n",
        "# Test different C values\n",
        "C_values = [0.1, 1, 10, 100]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Effect of C Parameter on SVM Decision Boundary', fontsize=16)\n",
        "\n",
        "results = {}\n",
        "\n",
        "for i, C in enumerate(C_values):\n",
        "    # Create and train SVM\n",
        "    svm = SVC(kernel='rbf', C=C, gamma='scale')\n",
        "    svm.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions and calculate accuracy\n",
        "    y_pred = svm.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    # Store results\n",
        "    results[f'C={C}'] = {\n",
        "        'accuracy': accuracy,\n",
        "        'n_support_vectors': len(svm.support_vectors_)\n",
        "    }\n",
        "    \n",
        "    # Plot decision boundary\n",
        "    row, col = i // 2, i % 2\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plot_svm_decision_boundary(X, y, svm, f'C = {C} (Acc: {accuracy:.3f}, SV: {len(svm.support_vectors_)})')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print results summary\n",
        "print(\"C Parameter Effect Summary:\")\n",
        "print(\"-\" * 40)\n",
        "for c_val, metrics in results.items():\n",
        "    print(f\"{c_val}: Accuracy = {metrics['accuracy']:.4f}, Support Vectors = {metrics['n_support_vectors']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q. 30 Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and after Laplace Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a dataset with some features having zero variance for demonstration\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=6, \n",
        "                          n_redundant=2, n_classes=3, random_state=42)\n",
        "\n",
        "# Add some extreme values to create potential zero probability scenarios\n",
        "# Make some features have very different scales\n",
        "X[:, 0] = X[:, 0] * 1000  # Scale up first feature\n",
        "X[:, 1] = X[:, 1] * 0.001  # Scale down second feature\n",
        "\n",
        "print(\"Dataset Information:\")\n",
        "print(f\"Shape: {X.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y))}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gaussian Naive Bayes without smoothing (default)\n",
        "gnb_default = GaussianNB()\n",
        "gnb_default.fit(X_train, y_train)\n",
        "y_pred_default = gnb_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Train Gaussian Naive Bayes with smoothing\n",
        "# Note: GaussianNB has a var_smoothing parameter (default 1e-9)\n",
        "gnb_smoothed = GaussianNB(var_smoothing=1e-5)  # Increased smoothing\n",
        "gnb_smoothed.fit(X_train, y_train)\n",
        "y_pred_smoothed = gnb_smoothed.predict(X_test)\n",
        "accuracy_smoothed = accuracy_score(y_test, y_pred_smoothed)\n",
        "\n",
        "print(\"\\nResults Comparison:\")\n",
        "print(f\"Accuracy without explicit smoothing: {accuracy_default:.4f}\")\n",
        "print(f\"Accuracy with increased smoothing: {accuracy_smoothed:.4f}\")\n",
        "\n",
        "# Compare variances calculated by both models\n",
        "print(\"\\nVariance comparison for first feature across classes:\")\n",
        "print(\"Default model variances:\", gnb_default.sigma_[0, 0], gnb_default.sigma_[1, 0], gnb_default.sigma_[2, 0])\n",
        "print(\"Smoothed model variances:\", gnb_smoothed.sigma_[0, 0], gnb_smoothed.sigma_[1, 0], gnb_smoothed.sigma_[2, 0])\n",
        "\n",
        "# Detailed classification reports\n",
        "print(\"\\nClassification Report - Default:\")\n",
        "print(classification_report(y_test, y_pred_default))\n",
        "\n",
        "print(\"\\nClassification Report - Smoothed:\")\n",
        "print(classification_report(y_test, y_pred_smoothed))\n",
        "\n",
        "# Test with different smoothing values\n",
        "smoothing_values = [1e-9, 1e-7, 1e-5, 1e-3, 1e-1]\n",
        "accuracies = []\n",
        "\n",
        "for smooth_val in smoothing_values:\n",
        "    gnb = GaussianNB(var_smoothing=smooth_val)\n",
        "    gnb.fit(X_train, y_train)\n",
        "    y_pred = gnb.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"Smoothing {smooth_val}: Accuracy = {acc:.4f}\")\n",
        "\n",
        "# Plot the effect of smoothing\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(smoothing_values, accuracies, 'bo-')\n",
        "plt.xlabel('Smoothing Parameter (log scale)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Effect of Laplace Smoothing on Gaussian Naive Bayes Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q. 31 Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "print(\"Dataset Information:\")\n",
        "print(f\"Shape: {X.shape}\")\n",
        "print(f\"Classes: {iris.target_names}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "}\n",
        "\n",
        "print(\"Parameter grid:\")\n",
        "for param, values in param_grid.items():\n",
        "    print(f\"{param}: {values}\")\n",
        "\n",
        "# Create SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# Perform GridSearchCV\n",
        "print(\"\\nPerforming GridSearchCV...\")\n",
        "start_time = time.time()\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=svm,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,  # Use all available cores\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the grid search\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"GridSearchCV completed in {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Get the best parameters\n",
        "print(\"\\nBest Parameters:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"Best Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Train the best model and evaluate on test set\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred = best_svm.predict(X_test_scaled)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test Accuracy with best parameters: {test_accuracy:.4f}\")\n",
        "\n",
        "# Compare with default SVM\n",
        "default_svm = SVC()\n",
        "default_svm.fit(X_train_scaled, y_train)\n",
        "y_pred_default = default_svm.predict(X_test_scaled)\n",
        "default_accuracy = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "print(f\"Test Accuracy with default parameters: {default_accuracy:.4f}\")\n",
        "\n",
        "# Display top 5 parameter combinations\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "top_5 = results_df.nlargest(5, 'mean_test_score')[['params', 'mean_test_score', 'std_test_score']]\n",
        "\n",
        "print(\"\\nTop 5 Parameter Combinations:\")\n",
        "for i, (idx, row) in enumerate(top_5.iterrows()):\n",
        "    print(f\"{i+1}. {row['params']} - Score: {row['mean_test_score']:.4f} (±{row['std_test_score']:.4f})\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nClassification Report (Best Model):\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q. 32 Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check if it improves accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,\n",
        "                          n_redundant=2, n_classes=2, weights=[0.1, 0.9],\n",
        "                          random_state=42)\n",
        "\n",
        "print(\"Dataset Information:\")\n",
        "print(f\"Shape: {X.shape}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "print(f\"Class 0: {np.sum(y == 0)} samples ({np.sum(y == 0)/len(y)*100:.1f}%)\")\n",
        "print(f\"Class 1: {np.sum(y == 1)} samples ({np.sum(y == 1)/len(y)*100:.1f}%)\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
        "                                                    random_state=42, stratify=y)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM without class weighting\n",
        "print(\"\\n1. SVM without class weighting:\")\n",
        "svm_no_weight = SVC(kernel='rbf', random_state=42)\n",
        "svm_no_weight.fit(X_train_scaled, y_train)\n",
        "y_pred_no_weight = svm_no_weight.predict(X_test_scaled)\n",
        "\n",
        "accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
        "print(f\"Accuracy: {accuracy_no_weight:.4f}\")\n",
        "\n",
        "# Train SVM with balanced class weighting\n",
        "print(\"\\n2. SVM with balanced class weighting:\")\n",
        "svm_balanced = SVC(kernel='rbf', class_weight='balanced', random_state=42)\n",
        "svm_balanced.fit(X_train_scaled, y_train)\n",
        "y_pred_balanced = svm_balanced.predict(X_test_scaled)\n",
        "\n",
        "accuracy_balanced = accuracy_score(y_test, y_pred_balanced)\n",
        "print(f\"Accuracy: {accuracy_balanced:.4f}\")\n",
        "\n",
        "# Calculate manual class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "print(f\"Computed class weights: {dict(zip(np.unique(y_train), class_weights))}\")\n",
        "\n",
        "# Train SVM with custom class weighting\n",
        "print(\"\\n3. SVM with custom class weighting:\")\n",
        "weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "svm_custom = SVC(kernel='rbf', class_weight=weight_dict, random_state=42)\n",
        "svm_custom.fit(X_train_scaled, y_train)\n",
        "y_pred_custom = svm_custom.predict(X_test_scaled)\n",
        "\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "print(f\"Accuracy: {accuracy_custom:.4f}\")\n",
        "\n",
        "# Compare classification reports\n",
        "print(\"\\nClassification Report - No Weighting:\")\n",
        "print(classification_report(y_test, y_pred_no_weight))\n",
        "\n",
        "print(\"\\nClassification Report - Balanced Weighting:\")\n",
        "print(classification_report(y_test, y_pred_balanced))\n",
        "\n",
        "# Visualize confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Confusion matrix for no weighting\n",
        "cm_no_weight = confusion_matrix(y_test, y_pred_no_weight)\n",
        "sns.heatmap(cm_no_weight, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
        "axes[0].set_title('Confusion Matrix - No Class Weighting')\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "\n",
        "# Confusion matrix for balanced weighting\n",
        "cm_balanced = confusion_matrix(y_test, y_pred_balanced)\n",
        "sns.heatmap(cm_balanced, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
        "axes[1].set_title('Confusion Matrix - Balanced Class Weighting')\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate and compare metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "models = {\n",
        "    'No Weighting': y_pred_no_weight,\n",
        "    'Balanced Weighting': y_pred_balanced,\n",
        "    'Custom Weighting': y_pred_custom\n",
        "}\n",
        "\n",
        "print(\"\\nDetailed Metrics Comparison:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Model':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for name, predictions in models.items():\n",
        "    acc = accuracy_score(y_test, predictions)\n",
        "    prec = precision_score(y_test, predictions, average='weighted')\n",
        "    rec = recall_score(y_test, predictions, average='weighted')\n",
        "    f1 = f1_score(y_test, predictions, average='weighted')\n",
        "    \n",
        "    print(f\"{name:<20} {acc:<10.4f} {prec:<10.4f} {rec:<10.4f} {f1:<10.4f}\")\n",
        "\n",
        "print(\"\\nConclusion:\")\n",
        "print(\"Class weighting helps balance the prediction between minority and majority classes,\")\n",
        "print(\"which is especially important for imbalanced datasets where accuracy alone can be misleading.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q. 33 Write a Python program to implement a Naïve Bayes classifier for spam detection using email data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Create a synthetic spam/ham email dataset for demonstration\n",
        "# In practice, you would load real email data\n",
        "spam_emails = [\n",
        "    \"Free money now! Click here to win $1000000\",\n",
        "    \"Congratulations! You've won a lottery! Claim now!\",\n",
        "    \"URGENT: Your account will be closed. Send money immediately\",\n",
        "    \"Get rich quick! Make money from home!\",\n",
        "    \"Amazing offer! Buy now and save 90%\",\n",
        "    \"Viagra cheap! No prescription needed\",\n",
        "    \"Weight loss pills! Lose 20 pounds in 2 weeks\",\n",
        "    \"Click here for free gifts and prizes\",\n",
        "    \"WINNER! You have been selected for cash prize\",\n",
        "    \"Increase your income! Work from home opportunity\"\n",
        "]\n",
        "\n",
        "ham_emails = [\n",
        "    \"Hi John, how are you doing today?\",\n",
        "    \"Meeting scheduled for tomorrow at 2 PM\",\n",
        "    \"Please review the attached document\",\n",
        "    \"Thank you for your help with the project\",\n",
        "    \"Can we reschedule our lunch meeting?\",\n",
        "    \"The quarterly report is ready for review\",\n",
        "    \"Happy birthday! Hope you have a great day\",\n",
        "    \"Conference call details attached\",\n",
        "    \"Please confirm your attendance\",\n",
        "    \"Looking forward to our discussion\"\n",
        "]\n",
        "\n",
        "# Create larger dataset by repeating and adding variations\n",
        "emails = spam_emails * 10 + ham_emails * 10\n",
        "labels = [1] * 100 + [0] * 100  # 1 for spam, 0 for ham\n",
        "\n",
        "# Add some variations to make dataset more realistic\n",
        "additional_spam = [\n",
        "    \"Free trial! No credit card required!\",\n",
        "    \"Limited time offer! Act now!\",\n",
        "    \"Cash prize waiting for you!\",\n",
        "    \"Miracle cure discovered by doctors!\",\n",
        "    \"Make thousands working from home!\"\n",
        "] * 6\n",
        "\n",
        "additional_ham = [\n",
        "    \"Project deadline is next week\",\n",
        "    \"Thanks for the presentation today\",\n",
        "    \"Family dinner this Sunday\",\n",
        "    \"Book club meeting postponed\",\n",
        "    \"Grocery list for this week\"\n",
        "] * 6\n",
        "\n",
        "emails.extend(additional_spam + additional_ham)\n",
        "labels.extend([1] * 30 + [0] * 30)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'email': emails,\n",
        "    'label': labels\n",
        "})\n",
        "\n",
        "print(\"Dataset Information:\")\n",
        "print(f\"Total emails: {len(df)}\")\n",
        "print(f\"Spam emails: {sum(df['label'] == 1)}\")\n",
        "print(f\"Ham emails: {sum(df['label'] == 0)}\")\n",
        "print(f\"Class distribution: {df['label'].value_counts()}\")\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df['processed_email'] = df['email'].apply(preprocess_text)\n",
        "\n",
        "# Split the data\n",
        "X = df['processed_email']\n",
        "y = df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
        "                                                    random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"Feature matrix shape: {X_train_tfidf.shape}\")\n",
        "\n",
        "# Train Multinomial Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = nb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
        "plt.title('Confusion Matrix - Spam Detection')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Show most informative features\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "log_probs = nb_classifier.feature_log_prob_\n",
        "\n",
        "# Get top features for spam\n",
        "spam_features_idx = log_probs[1].argsort()[-10:][::-1]\n",
        "spam_features = [(feature_names[i], log_probs[1][i]) for i in spam_features_idx]\n",
        "\n",
        "# Get top features for ham\n",
        "ham_features_idx = log_probs[0].argsort()[-10:][::-1]\n",
        "ham_features = [(feature_names[i], log_probs[0][i]) for i in ham_features_idx]\n",
        "\n",
        "print(\"\\nTop 10 features for SPAM:\")\n",
        "for feature, prob in spam_features:\n",
        "    print(f\"{feature}: {prob:.4f}\")\n",
        "\n",
        "print(\"\\nTop 10 features for HAM:\")\n",
        "for feature, prob in ham_features:\n",
        "    print(f\"{feature}: {prob:.4f}\")\n",
        "\n",
        "# Test with new emails\n",
        "test_emails = [\n",
        "    \"Congratulations! You won $1000! Click now!\",\n",
        "    \"Meeting scheduled for next Monday at 10 AM\",\n",
        "    \"Free money! No strings attached!\",\n",
        "    \"Please send the report by end of day\"\n",
        "]\n",
        "\n",
        "test_processed = [preprocess_text(email) for email in test_emails]\n",
        "test_vectorized = vectorizer.transform(test_processed)\n",
        "predictions = nb_classifier.predict(test_vectorized)\n",
        "probabilities = nb_classifier.predict_proba(test_vectorized)\n",
        "\n",
        "print(\"\\nPredictions on new emails:\")\n",
        "for i, email in enumerate(test_emails):\n",
        "    pred_label = \"SPAM\" if predictions[i] == 1 else \"HAM\"\n",
        "    spam_prob = probabilities[i][1]\n",
        "    print(f\"Email: '{email}'\")\n",
        "    print(f\"Prediction: {pred_label} (Spam probability: {spam_prob:.4f})\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q. 34 Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare their accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "import time\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "print(\"Dataset Information:\")\n",
        "print(f\"Shape: {X.shape}\")\n",
        "print(f\"Features: {len(wine.feature_names)}\")\n",
        "print(f\"Classes: {wine.target_names}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
        "                                                    random_state=42, stratify=y)\n",
        "\n",
        "# Scale the features for SVM\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON: SVM vs Naive Bayes\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Train SVM Classifier\n",
        "print(\"\\n1. Training SVM Classifier...\")\n",
        "start_time = time.time()\n",
        "svm_classifier = SVC(kernel='rbf', random_state=42)\n",
        "svm_classifier.fit(X_train_scaled, y_train)\n",
        "svm_train_time = time.time() - start_time\n",
        "\n",
        "# SVM Predictions\n",
        "svm_pred = svm_classifier.predict(X_test_scaled)\n",
        "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
        "\n",
        "print(f\"SVM Training Time: {svm_train_time:.4f} seconds\")\n",
        "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
        "\n",
        "# Train Naive Bayes Classifier\n",
        "print(\"\\n2. Training Naive Bayes Classifier...\")\n",
        "start_time = time.time()\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)  # NB doesn't require scaling\n",
        "nb_train_time = time.time() - start_time\n",
        "\n",
        "# Naive Bayes Predictions\n",
        "nb_pred = nb_classifier.predict(X_test)\n",
        "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
        "\n",
        "print(f\"Naive Bayes Training Time: {nb_train_time:.4f} seconds\")\n",
        "print(f\"Naive Bayes Accuracy: {nb_accuracy:.4f}\")\n",
        "\n",
        "# Compare results\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*40)\n",
        "print(f\"{'Metric':<20} {'SVM':<12} {'Naive Bayes':<12}\")\n",
        "print(\"-\" * 44)\n",
        "print(f\"{'Accuracy':<20} {svm_accuracy:<12.4f} {nb_accuracy:<12.4f}\")\n",
        "print(f\"{'Training Time':<20} {svm_train_time:<12.4f} {nb_train_time:<12.4f}\")\n",
        "\n",
        "# Detailed classification reports\n",
        "print(\"\\nSVM Classification Report:\")\n",
        "print(classification_report(y_test, svm_pred, target_names=wine.target_names))\n",
        "\n",
        "print(\"\\nNaive Bayes Classification Report:\")\n",
        "print(classification_report(y_test, nb_pred, target_names=wine.target_names))\n",
        "\n",
        "# Confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# SVM Confusion Matrix\n",
        "cm_svm = confusion_matrix(y_test, svm_pred)\n",
        "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=wine.target_names, yticklabels=wine.target_names)\n",
        "axes[0].set_title(f'SVM Confusion Matrix\\nAccuracy: {svm_accuracy:.4f}')\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "\n",
        "# Naive Bayes Confusion Matrix\n",
        "cm_nb = confusion_matrix(y_test, nb_pred)\n",
        "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Reds', ax=axes[1],\n",
        "            xticklabels=wine.target_names, yticklabels=wine.target_names)\n",
        "axes[1].set_title(f'Naive Bayes Confusion Matrix\\nAccuracy: {nb_accuracy:.4f}')\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Q. 35: FEATURE SELECTION WITH NAIVE BAYES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Feature selection using SelectKBest\n",
        "selector = SelectKBest(score_func=chi2, k=5)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features = selector.get_support(indices=True)\n",
        "selected_feature_names = [wine.feature_names[i] for i in selected_features]\n",
        "\n",
        "print(\"Selected Features:\")\n",
        "for i, feature in enumerate(selected_feature_names):\n",
        "    print(f\"{i+1}. {feature}\")\n",
        "\n",
        "# Train Naive Bayes with selected features\n",
        "nb_selected = GaussianNB()\n",
        "nb_selected.fit(X_train_selected, y_train)\n",
        "nb_selected_pred = nb_selected.predict(X_test_selected)\n",
        "nb_selected_accuracy = accuracy_score(y_test, nb_selected_pred)\n",
        "\n",
        "print(f\"\\nNaive Bayes with all features: {nb_accuracy:.4f}\")\n",
        "print(f\"Naive Bayes with selected features: {nb_selected_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Q. 36: ONE-VS-REST vs ONE-VS-ONE STRATEGIES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# One-vs-Rest (OvR) Strategy\n",
        "svm_ovr = SVC(kernel='rbf', decision_function_shape='ovr', random_state=42)\n",
        "svm_ovr.fit(X_train_scaled, y_train)\n",
        "ovr_pred = svm_ovr.predict(X_test_scaled)\n",
        "ovr_accuracy = accuracy_score(y_test, ovr_pred)\n",
        "\n",
        "# One-vs-One (OvO) Strategy  \n",
        "svm_ovo = SVC(kernel='rbf', decision_function_shape='ovo', random_state=42)\n",
        "svm_ovo.fit(X_train_scaled, y_train)\n",
        "ovo_pred = svm_ovo.predict(X_test_scaled)\n",
        "ovo_accuracy = accuracy_score(y_test, ovo_pred)\n",
        "\n",
        "print(f\"One-vs-Rest (OvR) Accuracy: {ovr_accuracy:.4f}\")\n",
        "print(f\"One-vs-One (OvO) Accuracy: {ovo_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Q. 37: KERNEL COMPARISON ON BREAST CANCER DATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "# Split and scale\n",
        "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer)\n",
        "\n",
        "scaler_cancer = StandardScaler()\n",
        "X_train_cancer_scaled = scaler_cancer.fit_transform(X_train_cancer)\n",
        "X_test_cancer_scaled = scaler_cancer.transform(X_test_cancer)\n",
        "\n",
        "# Test different kernels\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "kernel_results = {}\n",
        "\n",
        "for kernel in kernels:\n",
        "    svm_kernel = SVC(kernel=kernel, random_state=42)\n",
        "    svm_kernel.fit(X_train_cancer_scaled, y_train_cancer)\n",
        "    pred = svm_kernel.predict(X_test_cancer_scaled)\n",
        "    accuracy = accuracy_score(y_test_cancer, pred)\n",
        "    kernel_results[kernel] = accuracy\n",
        "    print(f\"{kernel.upper()} Kernel Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Find best kernel\n",
        "best_kernel = max(kernel_results, key=kernel_results.get)\n",
        "print(f\"\\nBest Kernel: {best_kernel.upper()} with accuracy {kernel_results[best_kernel]:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Q. 38: STRATIFIED K-FOLD CROSS-VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "svm_cv = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(svm_cv, X_train_scaled, y_train, cv=skf, scoring='accuracy')\n",
        "\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "print(f\"Average CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "print(f\"Individual fold scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
        "\n",
        "# Visualize CV scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, 6), cv_scores, alpha=0.7, color='skyblue', edgecolor='navy')\n",
        "plt.axhline(y=cv_scores.mean(), color='red', linestyle='--', \n",
        "            label=f'Mean: {cv_scores.mean():.4f}')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Stratified K-Fold Cross-Validation Results')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q. 39 Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
        "                           precision_score, recall_score, f1_score, log_loss,\n",
        "                           mean_absolute_error, roc_auc_score, roc_curve,\n",
        "                           precision_recall_curve, average_precision_score)\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Q. 39: NAIVE BAYES WITH DIFFERENT PRIOR PROBABILITIES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Default Naive Bayes (learns priors from data)\n",
        "nb_default = GaussianNB()\n",
        "nb_default.fit(X_train, y_train)\n",
        "pred_default = nb_default.predict(X_test)\n",
        "acc_default = accuracy_score(y_test, pred_default)\n",
        "\n",
        "print(f\"Default priors (learned from data): {nb_default.class_prior_}\")\n",
        "print(f\"Default accuracy: {acc_default:.4f}\")\n",
        "\n",
        "# Custom uniform priors\n",
        "uniform_priors = [1/3, 1/3, 1/3]\n",
        "nb_uniform = GaussianNB(priors=uniform_priors)\n",
        "nb_uniform.fit(X_train, y_train)\n",
        "pred_uniform = nb_uniform.predict(X_test)\n",
        "acc_uniform = accuracy_score(y_test, pred_uniform)\n",
        "\n",
        "print(f\"Uniform priors: {uniform_priors}\")\n",
        "print(f\"Uniform priors accuracy: {acc_uniform:.4f}\")\n",
        "\n",
        "# Custom biased priors (favor class 0)\n",
        "biased_priors = [0.6, 0.2, 0.2]\n",
        "nb_biased = GaussianNB(priors=biased_priors)\n",
        "nb_biased.fit(X_train, y_train)\n",
        "pred_biased = nb_biased.predict(X_test)\n",
        "acc_biased = accuracy_score(y_test, pred_biased)\n",
        "\n",
        "print(f\"Biased priors: {biased_priors}\")\n",
        "print(f\"Biased priors accuracy: {acc_biased:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Q. 40: RECURSIVE FEATURE ELIMINATION (RFE) WITH SVM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load Wine dataset for feature selection\n",
        "wine = datasets.load_wine()\n",
        "X_wine = wine.data\n",
        "y_wine = wine.target\n",
        "\n",
        "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(\n",
        "    X_wine, y_wine, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_wine_scaled = scaler.fit_transform(X_train_wine)\n",
        "X_test_wine_scaled = scaler.transform(X_test_wine)\n",
        "\n",
        "# SVM without feature selection\n",
        "svm_full = SVC(kernel='rbf', random_state=42)\n",
        "svm_full.fit(X_train_wine_scaled, y_train_wine)\n",
        "pred_full = svm_full.predict(X_test_wine_scaled)\n",
        "acc_full = accuracy_score(y_test_wine, pred_full)\n",
        "\n",
        "print(f\"SVM with all features ({X_wine.shape[1]} features): {acc_full:.4f}\")\n",
        "\n",
        "# RFE with different number of features\n",
        "n_features_list = [5, 8, 10, 13]\n",
        "rfe_results = {}\n",
        "\n",
        "for n_features in n_features_list:\n",
        "    # Create RFE selector\n",
        "    svm_rfe = SVC(kernel='linear', random_state=42)  # Linear for RFE\n",
        "    rfe = RFE(estimator=svm_rfe, n_features_to_select=n_features)\n",
        "    \n",
        "    # Fit RFE and transform data\n",
        "    X_train_rfe = rfe.fit_transform(X_train_wine_scaled, y_train_wine)\n",
        "    X_test_rfe = rfe.transform(X_test_wine_scaled)\n",
        "    \n",
        "    # Train final SVM with selected features\n",
        "    svm_final = SVC(kernel='rbf', random_state=42)\n",
        "    svm_final.fit(X_train_rfe, y_train_wine)\n",
        "    pred_rfe = svm_final.predict(X_test_rfe)\n",
        "    acc_rfe = accuracy_score(y_test_wine, pred_rfe)\n",
        "    \n",
        "    rfe_results[n_features] = acc_rfe\n",
        "    print(f\"SVM with RFE ({n_features} features): {acc_rfe:.4f}\")\n",
        "    \n",
        "    # Show selected features\n",
        "    selected_features = [wine.feature_names[i] for i in range(len(wine.feature_names)) if rfe.support_[i]]\n",
        "    print(f\"Selected features: {selected_features[:3]}...\" if len(selected_features) > 3 else f\"Selected features: {selected_features}\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Q. 41: SVM EVALUATION WITH PRECISION, RECALL, F1-SCORE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load breast cancer dataset for binary classification\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler_cancer = StandardScaler()\n",
        "X_train_cancer_scaled = scaler_cancer.fit_transform(X_train_cancer)\n",
        "X_test_cancer_scaled = scaler_cancer.transform(X_test_cancer)\n",
        "\n",
        "# Train SVM\n",
        "svm_cancer = SVC(kernel='rbf', random_state=42)\n",
        "svm_cancer.fit(X_train_cancer_scaled, y_train_cancer)\n",
        "pred_cancer = svm_cancer.predict(X_test_cancer_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test_cancer, pred_cancer)\n",
        "precision = precision_score(y_test_cancer, pred_cancer)\n",
        "recall = recall_score(y_test_cancer, pred_cancer)\n",
        "f1 = f1_score(y_test_cancer, pred_cancer)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Q. 42: NAIVE BAYES WITH LOG LOSS EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use probability predictions for log loss\n",
        "nb_cancer = GaussianNB()\n",
        "nb_cancer.fit(X_train_cancer, y_train_cancer)\n",
        "pred_proba_cancer = nb_cancer.predict_proba(X_test_cancer)\n",
        "\n",
        "# Calculate log loss\n",
        "log_loss_score = log_loss(y_test_cancer, pred_proba_cancer)\n",
        "print(f\"Log Loss (Cross-Entropy Loss): {log_loss_score:.4f}\")\n",
        "\n",
        "# Compare with accuracy\n",
        "pred_cancer_nb = nb_cancer.predict(X_test_cancer)\n",
        "acc_cancer_nb = accuracy_score(y_test_cancer, pred_cancer_nb)\n",
        "print(f\"Accuracy: {acc_cancer_nb:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Q. 43: SVM CONFUSION MATRIX VISUALIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test_cancer, pred_cancer)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=cancer.target_names,\n",
        "            yticklabels=cancer.target_names)\n",
        "plt.title('SVM Confusion Matrix - Breast Cancer Dataset')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Q. 44: SVR WITH MEAN ABSOLUTE ERROR (MAE)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load housing dataset for regression\n",
        "housing = fetch_california_housing()\n",
        "X_housing = housing.data[:1000]  # Use subset for faster computation\n",
        "y_housing = housing.target[:1000]\n",
        "\n",
        "X_train_housing, X_test_housing, y_train_housing, y_test_housing = train_test_split(\n",
        "    X_housing, y_housing, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features for SVR\n",
        "scaler_housing = StandardScaler()\n",
        "X_train_housing_scaled = scaler_housing.fit_transform(X_train_housing)\n",
        "X_test_housing_scaled = scaler_housing.transform(X_test_housing)\n",
        "\n",
        "# Train SVR\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train_housing_scaled, y_train_housing)\n",
        "pred_housing = svr.predict(X_test_housing_scaled)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test_housing, pred_housing)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "\n",
        "# Compare with some actual vs predicted values\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(5):\n",
        "    print(f\"Actual: {y_test_housing.iloc[i]:.2f}, Predicted: {pred_housing[i]:.2f}, Error: {abs(y_test_housing.iloc[i] - pred_housing[i]):.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Q. 45: NAIVE BAYES WITH ROC-AUC SCORE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Calculate ROC-AUC for Naive Bayes\n",
        "roc_auc = roc_auc_score(y_test_cancer, pred_proba_cancer[:, 1])\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test_cancer, pred_proba_cancer[:, 1])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Naive Bayes Classifier')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Q. 46: SVM PRECISION-RECALL CURVE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get probability scores for SVM (need to enable probability)\n",
        "svm_prob = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm_prob.fit(X_train_cancer_scaled, y_train_cancer)\n",
        "pred_proba_svm = svm_prob.predict_proba(X_test_cancer_scaled)\n",
        "\n",
        "# Calculate precision-recall curve\n",
        "precision_vals, recall_vals, _ = precision_recall_curve(y_test_cancer, pred_proba_svm[:, 1])\n",
        "average_precision = average_precision_score(y_test_cancer, pred_proba_svm[:, 1])\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall_vals, precision_vals, color='blue', lw=2, \n",
        "         label=f'Precision-Recall curve (AP = {average_precision:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - SVM Classifier')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Average Precision Score: {average_precision:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ASSIGNMENT COMPLETED!\")\n",
        "print(\"=\"*60)\n",
        "print(\"All 46 questions have been implemented successfully!\")\n",
        "print(\"Topics covered:\")\n",
        "print(\"- SVM Classification and Regression\")\n",
        "print(\"- Naive Bayes variants (Gaussian, Multinomial, Bernoulli)\")\n",
        "print(\"- Hyperparameter tuning with GridSearchCV\")\n",
        "print(\"- Feature selection and dimensionality reduction\")\n",
        "print(\"- Model evaluation metrics\")\n",
        "print(\"- Cross-validation techniques\")\n",
        "print(\"- Imbalanced dataset handling\")\n",
        "print(\"- Text classification and spam detection\")\n",
        "print(\"- Visualization techniques\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
